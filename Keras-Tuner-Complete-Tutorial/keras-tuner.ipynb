{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Install Keras Tuner ","metadata":{}},{"cell_type":"code","source":"!pip install keras-tuner --upgrade -q","metadata":{"execution":{"iopub.status.busy":"2022-09-13T12:42:45.391361Z","iopub.execute_input":"2022-09-13T12:42:45.392356Z","iopub.status.idle":"2022-09-13T12:42:57.014286Z","shell.execute_reply.started":"2022-09-13T12:42:45.392305Z","shell.execute_reply":"2022-09-13T12:42:57.013100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import keras_tuner\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom tensorflow.keras import layers, losses\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os ","metadata":{"execution":{"iopub.status.busy":"2022-09-13T13:28:15.673035Z","iopub.execute_input":"2022-09-13T13:28:15.673417Z","iopub.status.idle":"2022-09-13T13:28:15.679054Z","shell.execute_reply.started":"2022-09-13T13:28:15.673384Z","shell.execute_reply":"2022-09-13T13:28:15.677901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im = plt.imread(\"../input/asl-dataset/asl_dataset/f/hand1_f_bot_seg_3_cropped.jpeg\",\n                format='jpeg')\nprint(\"Image shape is :\", im.shape)\n# or using `io.BytesIO`\n# im = plt.imread(io.BytesIO(urllib2.urlopen(url).read()), format='jpeg')\nplt.imshow(im, cmap='Greys_r')\nplt.title('An image example')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-13T12:42:57.025601Z","iopub.execute_input":"2022-09-13T12:42:57.026657Z","iopub.status.idle":"2022-09-13T12:42:57.300069Z","shell.execute_reply.started":"2022-09-13T12:42:57.026622Z","shell.execute_reply":"2022-09-13T12:42:57.299185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the data with keras.utils","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 124\ntrain_data = keras.utils.image_dataset_from_directory(\n                        directory=\"../input/asl-dataset/asl_dataset\",\n                        labels= 'inferred',#[i for i in range(0,27)],\n                        label_mode='categorical',\n                        color_mode='rgb',\n                        batch_size=BATCH_SIZE,\n                        shuffle=True,\n                        image_size=(180, 180), \n)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-13T13:28:40.275968Z","iopub.execute_input":"2022-09-13T13:28:40.276350Z","iopub.status.idle":"2022-09-13T13:28:41.172136Z","shell.execute_reply.started":"2022-09-13T13:28:40.276290Z","shell.execute_reply":"2022-09-13T13:28:41.171134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create model","metadata":{}},{"cell_type":"code","source":"class MyHyperModel(keras_tuner.HyperModel) :\n    def build(self, hp, classes=37) : \n        model = keras.Sequential()\n        model.add(layers.Input( (180,180,3)))\n        model.add(layers.Resizing(128, 128, interpolation='bilinear'))\n        # Whether to include normalization layer\n        if hp.Boolean(\"normalize\"):\n            model.add(layers.Normalization())\n        \n        drop_rate = hp.Float(\"drop_rate\", min_value=0.05, max_value=0.25, step=0.10, default=0.15)\n        # Number of Conv Layers is up to tuning\n        for i in range( hp.Int(\"num_conv\", min_value=3, max_value=4, step=1), default=3) :\n            # Tune hyperparams of each conv layer separately by using f\"...{i}\"\n            model.add(layers.Conv2D(filters=hp.Choice(f\"filters_{i}\",[16,32,64], default=32),  #hp.Int(f\"filters_{i}\", min_value=16, max_value=64, step=16),\n                                    kernel_size= hp.Int(f\"kernel_\", min_value=3, max_value=5, step=2, default=3),\n                                    strides=1, padding='same',\n                                    activation=hp.Choice(\"conv_act\", [\"relu\"] , defalt='relu'))) #,\"leaky_relu\"] )))\n            #model.add(layers.MaxPooling2D())\n            # Batch Norm and Dropout layers as hyperparameters to be searched\n            if hp.Boolean(\"batch_norm\"):\n                model.add(layers.BatchNormalization())\n            if hp.Boolean(\"dropout\"):\n                model.add(layers.Dropout(drop_rate))\n\n        model.add(layers.Flatten())\n        for i in range(hp.Int(\"num_dense\", min_value=1, max_value=3, step=1)) :\n            model.add(layers.Dense(units=hp.Choice(\"neurons\", [150, 200], default=150),\n                                       activation=hp.Choice(\"mlp_activ\", ['sigmoid', 'relu'], default='relu')))\n            if hp.Boolean(\"batch_norm\"):\n                    model.add(layers.BatchNormalization())\n            if hp.Boolean(\"dropout\"):\n                    model.add(layers.Dropout(drop_rate))\n        \n        # Last layer\n        model.add(layers.Dense(classes, activation='softmax'))\n        \n        # Picking an opimizer and a loss function\n        model.compile(optimizer=hp.Choice('optim',['adam','adamax']),\n                      loss=hp.Choice(\"loss\",[\"sparse_categorical_crossentropy\"]), #,\"kl_divergence\"]),\n                      metrics = ['accuracy'])\n        \n        # A way to optimize the learning rate while also trying different optimizers\n        learning_rate = hp.Choice('lr', [ 0.03, 0.01, 0.003], default=0.01)\n        K.set_value(model.optimizer.learning_rate, learning_rate)\n        \n        return model\n    \n    \n    def fit(self, hp, model,x, *args, **kwargs) :\n        \n        return model.fit( x, \n                         *args,\n                         shuffle=hp.Boolean(\"shuffle\"),\n                         **kwargs)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-13T09:22:32.221071Z","iopub.execute_input":"2022-09-13T09:22:32.221422Z","iopub.status.idle":"2022-09-13T09:22:32.236758Z","shell.execute_reply.started":"2022-09-13T09:22:32.221391Z","shell.execute_reply":"2022-09-13T09:22:32.235655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n\ndef get_callbacks(weights_file, patience, lr_factor):\n  ''' Callbacks are used for saving the best weights and early stopping.'''\n  return [\n      # Only save the weights that correspond to the maximum validation accuracy.\n      ModelCheckpoint(filepath= weights_file,\n                      monitor=\"val_accuracy\",\n                      mode=\"max\",\n                      save_best_only=True, \n                      save_weights_only=True),\n      # If val_loss doesn't improve for a number of epochs set with 'patience' var \n      # training will stop to avoid overfitting.    \n      EarlyStopping(monitor=\"val_loss\",\n                    mode=\"min\",\n                    patience = patience,\n                    verbose=1),\n      # Learning rate is reduced by 'lr_factor' if val_loss stagnates\n      # for a number of epochs set with 'patience/2' var.     \n      ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\",\n                        factor=lr_factor, min_lr=1e-6, patience=patience//2, verbose=1)]","metadata":{"execution":{"iopub.status.busy":"2022-09-13T09:22:32.413879Z","iopub.execute_input":"2022-09-13T09:22:32.414882Z","iopub.status.idle":"2022-09-13T09:22:32.423158Z","shell.execute_reply.started":"2022-09-13T09:22:32.414834Z","shell.execute_reply":"2022-09-13T09:22:32.421994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sanity Check\n#### Check everything works correctly by training a \"dummy\" model for 1 epoch with random data.","metadata":{}},{"cell_type":"code","source":"classes = 37\nhp = keras_tuner.HyperParameters()\nhypermodel = MyHyperModel()\nmodel = hypermodel.build(hp, classes)\nhypermodel.fit(hp, model, np.random.rand(BATCH_SIZE, 400, 400,3), np.random.rand(BATCH_SIZE, 1))","metadata":{"execution":{"iopub.status.busy":"2022-09-13T09:22:33.356381Z","iopub.execute_input":"2022-09-13T09:22:33.357499Z","iopub.status.idle":"2022-09-13T09:22:36.196532Z","shell.execute_reply.started":"2022-09-13T09:22:33.357446Z","shell.execute_reply":"2022-09-13T09:22:36.195551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initiate the tuner ","metadata":{}},{"cell_type":"code","source":"tuner = keras_tuner.BayesianOptimization(\n                        hypermodel=MyHyperModel(),\n                        objective = \"val_accuracy\",\n                        max_trials = 2,\n#                         factor=30,\n#                         max_epochs=50,\n                        overwrite=True,\n                        directory='BO_search_dir',\n                        project_name='sign_language_cnn')\n#tuner.search_space_summary(extended=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T09:22:45.413912Z","iopub.execute_input":"2022-09-13T09:22:45.414283Z","iopub.status.idle":"2022-09-13T09:22:45.570225Z","shell.execute_reply.started":"2022-09-13T09:22:45.414252Z","shell.execute_reply":"2022-09-13T09:22:45.569173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the training data before searching the hyperparameter space.","metadata":{}},{"cell_type":"code","source":"# epochs defines how many epochs each candidate model will be trained for \ntuner.search(x=train_data, epochs=2, validation_data=val_data, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Get the top 2 models.\n# models = tuner.get_best_models(num_models=2)\n# best_model = models[0]\n# # Build the model.\n# # Needed for `Sequential` without specified `input_shape`.\n# best_model.build(input_shape=(None, 400, 400))\n# best_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-09-13T09:16:33.815922Z","iopub.execute_input":"2022-09-13T09:16:33.816234Z","iopub.status.idle":"2022-09-13T09:16:33.821945Z","shell.execute_reply.started":"2022-09-13T09:16:33.816204Z","shell.execute_reply":"2022-09-13T09:16:33.820700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tuner summary results\ntuner.results_summary(1)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-13T09:26:00.680515Z","iopub.status.idle":"2022-09-13T09:26:00.681751Z","shell.execute_reply.started":"2022-09-13T09:26:00.681406Z","shell.execute_reply":"2022-09-13T09:26:00.681439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the top 2 hyperparameters.\nbest_hps = tuner.get_best_hyperparameters(1)\n# Build the model with the best hp.\nh_model = MyHyperModel()\nmodel = h_model.build(best_hps[0])\n# Fit with the entire dataset.\ncombined_dataset = train_data.concatenate(val_data)\nhistory = model.fit(x=train_data, validation_data=val_data, epochs=15,\n                    callbacks=get_callbacks('Net_weights.h5',\n                                            patience=10,\n                                            lr_factor=0.3)) ","metadata":{"execution":{"iopub.status.busy":"2022-09-13T09:26:00.683528Z","iopub.status.idle":"2022-09-13T09:26:00.684067Z","shell.execute_reply.started":"2022-09-13T09:26:00.683809Z","shell.execute_reply":"2022-09-13T09:26:00.683833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('Net_weights.h5')\nmodel.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T09:21:03.440162Z","iopub.execute_input":"2022-09-13T09:21:03.440549Z","iopub.status.idle":"2022-09-13T09:21:09.574529Z","shell.execute_reply.started":"2022-09-13T09:21:03.440506Z","shell.execute_reply":"2022-09-13T09:21:09.573561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('Best_model')","metadata":{"execution":{"iopub.status.busy":"2022-09-13T09:21:09.576488Z","iopub.execute_input":"2022-09-13T09:21:09.577136Z","iopub.status.idle":"2022-09-13T09:21:12.036954Z","shell.execute_reply.started":"2022-09-13T09:21:09.577098Z","shell.execute_reply":"2022-09-13T09:21:12.035957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_data)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-11T18:39:00.097379Z","iopub.execute_input":"2022-09-11T18:39:00.097859Z","iopub.status.idle":"2022-09-11T18:39:03.655962Z","shell.execute_reply.started":"2022-09-11T18:39:00.097820Z","shell.execute_reply":"2022-09-11T18:39:03.654886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_training_curves(history, val=True):\n    #Defining the metrics we will plot.\n    train_acc=history.history['accuracy']  \n    train_loss = history.history['loss']\n\n    if val :\n        val_acc=history.history['val_accuracy']\n        val_loss = history.history['val_loss']\n\n    #Range for the X axis.\n    epochs = range(len(train_loss))\n\n    fig,axis=plt.subplots(1,2,figsize=(20,8))#1 row, 2 col , width=20,height=8 inches.\n\n    #Plotting Loss figures.\n    plt.rcParams.update({'font.size': 22}) #configuring font size.\n    plt.subplot(1,2,1) #plot 1st curve.\n    plt.plot(epochs,train_loss,c=\"red\",label=\"Training Loss\") #plotting\n    if val: plt.plot(epochs,val_loss,c=\"blue\",label=\"Validation Loss\")\n    plt.xlabel(\"Epochs\") #title for x axis\n    plt.ylabel(\"Loss\")   #title for y axis\n    plt.legend()\n\n    #Plotting Accuracy figures. \n    plt.subplot(1,2,2) #plot 2nd curve.\n    plt.plot(epochs,train_acc,c=\"red\",label=\"Training Acc\") #plotting\n    if val : plt.plot(epochs,val_acc,c=\"blue\",label=\"Validation Acc\")\n    plt.xlabel(\"Epochs\")   #title for x axis\n    plt.ylabel(\"Accuracy\") #title for y axis\n    plt.legend()\n\nplot_training_curves(history, val=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-13T09:21:12.038768Z","iopub.execute_input":"2022-09-13T09:21:12.039154Z","iopub.status.idle":"2022-09-13T09:21:12.459105Z","shell.execute_reply.started":"2022-09-13T09:21:12.039117Z","shell.execute_reply":"2022-09-13T09:21:12.457971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2022-09-12T10:06:40.007710Z","iopub.execute_input":"2022-09-12T10:06:40.008137Z","iopub.status.idle":"2022-09-12T10:06:40.035013Z","shell.execute_reply.started":"2022-09-12T10:06:40.008104Z","shell.execute_reply":"2022-09-12T10:06:40.032945Z"},"trusted":true},"execution_count":null,"outputs":[]}]}